{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print digits.target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   0.   5.  13.   9.   1.   0.   0.]\n",
      " [  0.   0.  13.  15.  10.  15.   5.   0.]\n",
      " [  0.   3.  15.   2.   0.  11.   8.   0.]\n",
      " [  0.   4.  12.   0.   0.   8.   8.   0.]\n",
      " [  0.   5.   8.   0.   0.   9.   8.   0.]\n",
      " [  0.   4.  11.   0.   1.  12.   7.   0.]\n",
      " [  0.   2.  14.   5.  10.  12.   0.   0.]\n",
      " [  0.   0.   6.  13.  10.   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "print digits.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Vector [[  0.   0.   5.  13.   9.   1.   0.   0.   0.   0.  13.  15.  10.  15.\n",
      "    5.   0.   0.   3.  15.   2.   0.  11.   8.   0.   0.   4.  12.   0.\n",
      "    0.   8.   8.   0.   0.   5.   8.   0.   0.   9.   8.   0.   0.   4.\n",
      "   11.   0.   1.  12.   7.   0.   0.   2.  14.   5.  10.  12.   0.   0.\n",
      "    0.   0.   6.  13.  10.   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "print 'Feature Vector', digits.images[0].reshape(-1, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting points of interest as features\n",
    "\n",
    "points of interest, are points that are surrounded by rich textures and can be reproduced despite perturbing the images. ```Edges and corners``` are two common types of points of interest.\n",
    "\n",
    "# Scale-Invariant Feature Transform\n",
    "\n",
    "SIFT is a method for extracting features from an image that is less sensitive to the scale, rotation, and illumination of the image than the extraction methods we have previously discussed.\n",
    "\n",
    "# Speeded-Up Robust Features \n",
    "\n",
    "SURF is another method of extracting interesting points of an image and creating descriptions that are invariant of the image's scale, orientation, and illumination. SURF can be computed more quickly than SIFT, and it is more effective at recognizing features across images that have been transformed in certain ways.\n",
    "\n",
    "# Data standardization\n",
    "Many estimators perform better when they are trained on standardized data sets. Standardized data has zero mean and unit variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -0.70710678 -1.38873015  0.52489066  0.59299945 -1.35873244]\n",
      " [ 0.         -0.70710678  0.46291005  0.87481777  0.81537425  1.01904933]\n",
      " [ 0.          1.41421356  0.9258201  -1.39970842 -1.4083737   0.33968311]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "X = np.array([\n",
    "[0., 0., 5., 13., 9., 1.],\n",
    "[0., 0., 13., 15., 10., 15.],\n",
    "[0., 3., 15., 2., 0., 11.]\n",
    "])\n",
    "print preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function scale in module sklearn.preprocessing.data:\n",
      "\n",
      "scale(X, axis=0, with_mean=True, with_std=True, copy=True)\n",
      "    Standardize a dataset along any axis\n",
      "    \n",
      "    Center to the mean and component wise scale to unit variance.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : {array-like, sparse matrix}\n",
      "        The data to center and scale.\n",
      "    \n",
      "    axis : int (0 by default)\n",
      "        axis used to compute the means and standard deviations along. If 0,\n",
      "        independently standardize each feature, otherwise (if 1) standardize\n",
      "        each sample.\n",
      "    \n",
      "    with_mean : boolean, True by default\n",
      "        If True, center the data before scaling.\n",
      "    \n",
      "    with_std : boolean, True by default\n",
      "        If True, scale the data to unit variance (or equivalently,\n",
      "        unit standard deviation).\n",
      "    \n",
      "    copy : boolean, optional, default True\n",
      "        set to False to perform inplace row normalization and avoid a\n",
      "        copy (if the input is already a numpy array or a scipy.sparse\n",
      "        CSC matrix and if axis is 1).\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    This implementation will refuse to center scipy.sparse matrices\n",
      "    since it would make them non-sparse and would potentially crash the\n",
      "    program with memory exhaustion problems.\n",
      "    \n",
      "    Instead the caller is expected to either set explicitly\n",
      "    `with_mean=False` (in that case, only variance scaling will be\n",
      "    performed on the features of the CSC matrix) or to call `X.toarray()`\n",
      "    if he/she expects the materialized dense array to fit in memory.\n",
      "    \n",
      "    To avoid memory copy the caller should pass a CSC matrix.\n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    StandardScaler: Performs scaling to unit variance using the``Transformer`` API\n",
      "        (e.g. as part of a preprocessing :class:`sklearn.pipeline.Pipeline`).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(preprocessing.scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
